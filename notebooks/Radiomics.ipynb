{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Models Based on Radiomics Features\n",
    "\n",
    "This notebook contains the code for extracting radiomics features from the ultrasound data. Random forest classifiers are trained to predict the specified target variable from the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T08:39:02.521587Z",
     "start_time": "2022-03-30T08:39:00.478377Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import radiomics\n",
    "import SimpleITK as sitk\n",
    "from radiomics import featureextractor\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, f_classif\n",
    "from sklearn.metrics import (accuracy_score, auc, balanced_accuracy_score,\n",
    "                             confusion_matrix, f1_score, make_scorer,\n",
    "                             precision_recall_curve, roc_auc_score, det_curve)\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedGroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important constants\n",
    "# TODO: fill in relevant directories\n",
    "\n",
    "# Flag indicating whether to run k-fold CV: if False, models are evaluated using train-test split\n",
    "VALIDATE = False\n",
    "# Number of the folds for the CV\n",
    "K_FOLDS = 5\n",
    "SEED = 0\n",
    "# Directory for logging\n",
    "LOG_DIR = '...'\n",
    "# Directory with the preprocessed ultrasound images\n",
    "IMAGE_DIR = '...'\n",
    "# Directory with data dictionaries\n",
    "DICT_DIR = '...'\n",
    "# Configuration file with the parameters for extracting radiomics features\n",
    "RADIOMICS_CONFIG = './radiomics_params.yaml'\n",
    "# Target variable: 'diagnosis', 'treatment' or 'complications'\n",
    "TARGET_LABEL = 'diagnosis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T08:39:02.532654Z",
     "start_time": "2022-03-30T08:39:02.524741Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the PyRadiomics logger (default log-level = INFO)\n",
    "logger = radiomics.logger\n",
    "logger.setLevel(logging.DEBUG)  # set level to DEBUG to include debug log messages in log file\n",
    "\n",
    "# Write out all log entries to a file\n",
    "handler = logging.FileHandler(filename=os.path.join(LOG_DIR, 'testLog.txt'), mode='w')\n",
    "formatter = logging.Formatter('%(levelname)s:%(name)s: %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T08:39:07.768539Z",
     "start_time": "2022-03-30T08:39:02.542863Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training images and create masks\n",
    "# NOTE: the entire US image is used to ectract features, i.e. no ROI is specified\n",
    "image_dir = os.path.join(IMAGE_DIR, 'constant_padding/deepfilled_cropped_train')\n",
    "\n",
    "with open(os.path.join(DICT_DIR, TARGET_LABEL, 'imputed/final/app_data_train')) as f:\n",
    "    gen_labels = json.load(f)\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "groups = []\n",
    "masks = []\n",
    "\n",
    "for i in range(len(gen_labels)):\n",
    "    img_code = list(gen_labels)[i]\n",
    "    label = list(gen_labels.values())[i][1]\n",
    "    file_names = list(gen_labels.values())[i][0]\n",
    "    for file_name in file_names:\n",
    "        images.append(sitk.ReadImage(os.path.join(image_dir, file_name), sitk.sitkInt32))\n",
    "        labels.append([file_name, label])\n",
    "    groups.extend(np.repeat(int(img_code), len(file_names)))\n",
    "# Create a full mask\n",
    "# GetImageFromArray reverses axes!\n",
    "for i in range(len(images)):\n",
    "    mask_arr = np.ones(images[i].GetSize()[::-1])\n",
    "    mask_arr[0, 0] = 0 # pyradiomics wants mask to be segmented -> dummy segmentation\n",
    "    mask = sitk.GetImageFromArray(mask_arr)\n",
    "    mask.CopyInformation(images[i])  # copy geometric info\n",
    "    masks.append(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T08:39:07.776537Z",
     "start_time": "2022-03-30T08:39:07.770352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load test images and create masks\n",
    "if not VALIDATE:\n",
    "    image_dir_test = os.path.join(IMAGE_DIR, 'constant_padding/deepfilled_cropped_test')    \n",
    "    with open(os.path.join(DICT_DIR, TARGET_LABEL, 'imputed/final/app_data_test')) as f:\n",
    "        gen_labels_test = json.load(f)\n",
    "\n",
    "    images_test = []\n",
    "    labels_test = []\n",
    "    groups_test = []\n",
    "    masks_test = []\n",
    "    \n",
    "    for i in range(len(gen_labels_test)):\n",
    "        img_code = list(gen_labels_test)[i]\n",
    "        label = list(gen_labels_test.values())[i][1]\n",
    "        file_names = list(gen_labels_test.values())[i][0]\n",
    "        for file_name in file_names:\n",
    "            images_test.append(sitk.ReadImage(os.path.join(image_dir_test, file_name), sitk.sitkInt32))\n",
    "            labels_test.append([file_name, label])\n",
    "        groups_test.extend(np.repeat(int(img_code), len(file_names)))\n",
    "    # Create a full mask\n",
    "    # GetImageFromArray reverses axes!\n",
    "    for i in range(len(images_test)):\n",
    "        mask_arr = np.ones(images_test[i].GetSize()[::-1])\n",
    "        mask_arr[0, 0] = 0 # pyradiomics wants mask to be segmented -> dummy segmentation\n",
    "        mask = sitk.GetImageFromArray(mask_arr)\n",
    "        mask.CopyInformation(images_test[i])  # copy geometric info\n",
    "        masks_test.append(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T08:39:08.067035Z",
     "start_time": "2022-03-30T08:39:07.778654Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(sitk.GetArrayFromImage(images[10]), cmap='gray')\n",
    "plt.title('Abdomen')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(sitk.GetArrayFromImage(masks[10]))        \n",
    "plt.title('Mask')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radiomic features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T08:39:08.126649Z",
     "start_time": "2022-03-30T08:39:08.068797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use parameters from the configuration file\n",
    "extractor = featureextractor.RadiomicsFeatureExtractor(RADIOMICS_CONFIG)\n",
    "print('Extraction parameters:\\n\\t', extractor.settings)\n",
    "print('Enabled filters:\\n\\t', extractor.enabledImagetypes)\n",
    "print('Enabled features:\\n\\t', extractor.enabledFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T08:44:36.686860Z",
     "start_time": "2022-03-30T08:39:08.128284Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the design matrix\n",
    "feature_names = []\n",
    "X = []\n",
    "with tqdm(total=len(images)) as pbar:\n",
    "    for i in range(len(images)):\n",
    "        i_features = []\n",
    "        featureVector = extractor.execute(images[i], masks[i])    \n",
    "        for key, value in featureVector.items():\n",
    "            if i == 0 and 'diagnostics' not in key:\n",
    "                feature_names.append(key)\n",
    "            if 'diagnostics' not in key:        \n",
    "                i_features.append(float(value))\n",
    "        X.append(i_features)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Average per-view features for every subject\n",
    "X = np.array(X)\n",
    "for j in np.unique(groups):\n",
    "    j_inds = np.argwhere(groups == j)[:, 0]\n",
    "    X[j_inds] = np.tile(np.mean(X[j_inds], 0), (len(j_inds), 1))\n",
    "\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "labels_df = pd.DataFrame(labels, columns=[\"File name\", TARGET_LABEL])\n",
    "print(\"X shape:\", X_df.shape)\n",
    "\n",
    "if not VALIDATE:\n",
    "    X_test = []\n",
    "    with tqdm(total=len(images_test)) as pbar:\n",
    "        for i in range(len(images_test)):\n",
    "            i_features = []\n",
    "            featureVector = extractor.execute(images_test[i], masks_test[i])    \n",
    "            for key, value in featureVector.items():\n",
    "                if 'diagnostics' not in key:        \n",
    "                    i_features.append(float(value))\n",
    "            X_test.append(i_features)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Average per-view features for every subject\n",
    "    X_test = np.array(X_test)\n",
    "    for j in np.unique(groups_test):\n",
    "        j_inds = np.argwhere(groups_test == j)[:, 0]\n",
    "        X_test[j_inds] = np.tile(np.mean(X_test[j_inds], 0), (len(j_inds), 1))\n",
    "\n",
    "    X_df_test = pd.DataFrame(X_test, columns=feature_names)\n",
    "    labels_df_test = pd.DataFrame(labels_test, columns=[\"File name\", TARGET_LABEL])\n",
    "    print(\"X_test shape:\", X_df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T08:44:36.697375Z",
     "start_time": "2022-03-30T08:44:36.690566Z"
    }
   },
   "outputs": [],
   "source": [
    "def aggregate_scores(groups, scores, operation = 'most_common'):\n",
    "    # Aggregates predictions\n",
    "    scores_final = []\n",
    "    scores_grouped = []\n",
    "    \n",
    "    curr_group = groups[0]\n",
    "    curr_scores = [scores[0]]\n",
    "    for i in range(1, len(groups)):\n",
    "        if groups[i] == curr_group:\n",
    "            curr_scores.append(scores[i])\n",
    "            if i == len(groups)-1:\n",
    "                scores_grouped.append(curr_scores)\n",
    "        else:\n",
    "            scores_grouped.append(curr_scores)\n",
    "            curr_group = groups[i]\n",
    "            curr_scores = [scores[i]]\n",
    "            if i == len(groups)-1:\n",
    "                scores_grouped.append(curr_scores)\n",
    "                \n",
    "    assert len(scores_grouped) == len(np.unique(groups))\n",
    "    if operation == 'most_common':\n",
    "        scores_final = [np.argmax(np.bincount(np.array(group))) for group in scores_grouped]\n",
    "    else:\n",
    "        scores_final = [operation(group) for group in scores_grouped]\n",
    "    scores_final = np.array(scores_final)\n",
    "    \n",
    "    assert len(scores_final) == len(np.unique(groups))\n",
    "    return scores_final "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T08:44:36.878790Z",
     "start_time": "2022-03-30T08:44:36.699925Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fair coin flip\n",
    "if VALIDATE:\n",
    "    results = np.empty((K_FOLDS, 6)) \n",
    "else:\n",
    "    accuracy = 0.0\n",
    "    f1_macro = 0.0\n",
    "    bal_acc = 0.0\n",
    "    auroc = 0.0\n",
    "    aupr = 0.0\n",
    "    \n",
    "np.random.seed(SEED)\n",
    "B = 500\n",
    "\n",
    "for b in range(B):\n",
    "    kfold = StratifiedGroupKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(X_df, labels_df[TARGET_LABEL], groups)):  \n",
    "        if fold > 0 and not VALIDATE:\n",
    "            break\n",
    "        if VALIDATE:\n",
    "            y_val = labels_df[TARGET_LABEL].loc[val_ids] \n",
    "            groups_val = np.array(groups)[val_ids]\n",
    "\n",
    "            y_val_prob = np.random.uniform(size=len(y_val))\n",
    "            y_val_prob = aggregate_scores(groups_val, y_val_prob, operation = np.mean)\n",
    "            y_val_pred = (y_val_prob > 0.5)*1\n",
    "            y_val_true = aggregate_scores(groups_val, y_val.to_numpy(), operation = 'most_common') \n",
    "\n",
    "            accuracy = accuracy_score(y_val_true, y_val_pred)\n",
    "            f1_macro = f1_score(y_val_true, y_val_pred, average='macro', zero_division=0)\n",
    "            bal_acc = balanced_accuracy_score(y_val_true, y_val_pred)\n",
    "            auroc = roc_auc_score(y_val_true, y_val_prob)\n",
    "            precision, recall, thresholds = precision_recall_curve(y_val_true, y_val_prob)\n",
    "            aupr = auc(recall, precision)\n",
    "            brier_score = np.mean((y_val_prob - y_val_true)**2)\n",
    "            \n",
    "            results[fold, 0] += accuracy\n",
    "            results[fold, 1] += f1_macro\n",
    "            results[fold, 2] += bal_acc\n",
    "            results[fold, 3] += auroc\n",
    "            results[fold, 4] += aupr\n",
    "            results[fold, 5] += brier_score\n",
    "        else:\n",
    "            y_test_prob = np.random.uniform(size=len(labels_df_test))\n",
    "            y_test_prob = aggregate_scores(groups_test, y_test_prob, operation = np.mean)\n",
    "            y_test_pred = (y_test_prob > 0.5).astype(int)\n",
    "            y_test_true = aggregate_scores(groups_test, labels_df_test[TARGET_LABEL].to_numpy(), \n",
    "                                           operation = 'most_common')\n",
    "\n",
    "            accuracy += accuracy_score(y_test_true, y_test_pred)\n",
    "            f1_macro += f1_score(y_test_true, y_test_pred, average='macro', zero_division=0)\n",
    "            bal_acc += balanced_accuracy_score(y_test_true, y_test_pred)\n",
    "            auroc += roc_auc_score(y_test_true, y_test_prob)\n",
    "            precision_, recall_, thresholds_ = precision_recall_curve(y_test_true, y_test_prob)\n",
    "            aupr += auc(recall_, precision_)\n",
    "        \n",
    "metric_names = ['Accuracy', 'F1_macro', 'Balanced accuracy', 'AUROC', 'AUPR', 'Brier']\n",
    "if VALIDATE:\n",
    "    print(f'\\n{K_FOLDS}-FOLD CROSS VALIDATION RESULTS: DUMMY')\n",
    "    print()\n",
    "    agg_results_df = pd.DataFrame([np.mean(results, axis=0), np.std(results, axis=0)], columns=metric_names, \n",
    "                                  index=['mean', 'std'])\n",
    "    print(agg_results_df)      \n",
    "else:\n",
    "    results_df = pd.DataFrame([np.array([accuracy/B, f1_macro/B, bal_acc/B, auroc/B, aupr/B, brier/B])], \n",
    "                              columns=metric_names, index=['test set'])   \n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T08:46:26.367601Z",
     "start_time": "2022-03-30T08:44:36.880706Z"
    }
   },
   "outputs": [],
   "source": [
    "# RF classifier\n",
    "B = 10\n",
    "\n",
    "if VALIDATE:\n",
    "    results = np.empty((K_FOLDS, 11))\n",
    "    FP_codes_folds = []\n",
    "    FN_codes_folds = []\n",
    "else:\n",
    "    accuracy = np.zeros(B)\n",
    "    f1_macro = np.zeros(B)\n",
    "    bal_acc = np.zeros(B)\n",
    "    auroc = np.zeros(B)\n",
    "    aupr = np.zeros(B)\n",
    "    fpr_at_75 = np.zeros(B)\n",
    "    fpr_at_80 = np.zeros(B)\n",
    "    fpr_at_90 = np.zeros(B)\n",
    "    fpr_at_95 = np.zeros(B)\n",
    "    fpr_at_99 = np.zeros(B)\n",
    "    brier = np.zeros(B)\n",
    "\n",
    "for b in range(B):\n",
    "    kfold = StratifiedGroupKFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED+b)\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(X_df, labels_df[TARGET_LABEL], groups)):  \n",
    "        if fold > 0 and not VALIDATE:\n",
    "            break\n",
    "        if VALIDATE:\n",
    "            X_train = X_df.loc[train_ids]\n",
    "            y_train = labels_df.loc[train_ids]        \n",
    "            groups_train = np.array(groups)[train_ids]\n",
    "            X_val = X_df.loc[val_ids]\n",
    "            y_val = labels_df.loc[val_ids]\n",
    "            groups_val = np.array(groups)[val_ids]\n",
    "        else:\n",
    "            X_train = X_df\n",
    "            y_train = labels_df\n",
    "            groups_train = np.array(groups)\n",
    "\n",
    "        kfold_inner = StratifiedGroupKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            ('selector1', VarianceThreshold(threshold=0.0001)),\n",
    "            ('selector2', SelectKBest(f_classif)), \n",
    "            ('classifier', RandomForestClassifier(random_state=SEED+b, class_weight='balanced'))\n",
    "                        ])   \n",
    "        param_grid = {\n",
    "            'classifier__max_depth': [5, 10, 15], \n",
    "            'classifier__n_estimators': [100, 200],\n",
    "            'selector2__k': [20, 40, 60]\n",
    "                     }\n",
    "\n",
    "        search = GridSearchCV(pipe, param_grid, n_jobs=-1, scoring=make_scorer(f1_score, average='macro'), \n",
    "                              cv=kfold_inner.split(X_train, y_train[TARGET_LABEL], groups_train))\n",
    "\n",
    "        search.fit(X_train, y_train[TARGET_LABEL].values.ravel()) \n",
    "\n",
    "        if VALIDATE:\n",
    "            y_val_prob = search.predict_proba(X_val)[:,1]\n",
    "\n",
    "            y_val_prob = aggregate_scores(groups_val, y_val_prob, operation = np.mean)\n",
    "            y_val_pred = (y_val_prob > 0.5)*1\n",
    "\n",
    "            y_val_true = aggregate_scores(groups_val, y_val[TARGET_LABEL].to_numpy(), operation = 'most_common')\n",
    "            groups_val = aggregate_scores(groups_val, groups_val, operation = 'most_common')   \n",
    "\n",
    "            accuracy = accuracy_score(y_val_true, y_val_pred)\n",
    "            f1_macro = f1_score(y_val_true, y_val_pred, average='macro', zero_division=0)\n",
    "            bal_acc = balanced_accuracy_score(y_val_true, y_val_pred)\n",
    "            auroc = roc_auc_score(y_val_true, y_val_prob)\n",
    "            precision, recall, thresholds = precision_recall_curve(y_val_true, y_val_prob)\n",
    "            aupr = auc(recall, precision)\n",
    "\n",
    "            fpr, fnr, _ = det_curve(y_val_true, y_val_prob)\n",
    "            tpr = 1 - fnr\n",
    "            fpr_at_k = {}\n",
    "            ks = [0.75, 0.80, 0.90, 0.95, 0.99]\n",
    "            for k in ks:\n",
    "                ind = np.argmin(np.abs(tpr - k))\n",
    "                fpr_at_k['FPR at ' + str(k)] = np.round(fpr[ind], 3)\n",
    "            \n",
    "            brier_score = np.mean((y_val_prob - y_val_true)**2)\n",
    "            \n",
    "            results[fold, 0] = accuracy\n",
    "            results[fold, 1] = f1_macro\n",
    "            results[fold, 2] = bal_acc\n",
    "            results[fold, 3] = auroc\n",
    "            results[fold, 4] = aupr\n",
    "            results[fold, 5] = fpr_at_k['FPR at 0.75']\n",
    "            results[fold, 6] = fpr_at_k['FPR at 0.8']\n",
    "            results[fold, 7] = fpr_at_k['FPR at 0.9']\n",
    "            results[fold, 8] = fpr_at_k['FPR at 0.95']\n",
    "            results[fold, 9] = fpr_at_k['FPR at 0.99']\n",
    "            results[fold, 10] = brier_score\n",
    "\n",
    "            FP = np.intersect1d(np.argwhere(y_val_pred==1).flatten(), np.argwhere(y_val_true==0).flatten()) \n",
    "            FN = np.intersect1d(np.argwhere(y_val_pred==0).flatten(), np.argwhere(y_val_true==1).flatten())\n",
    "\n",
    "            FP_codes_folds.append(groups_val[FP])\n",
    "            FN_codes_folds.append(groups_val[FN])\n",
    "        else:\n",
    "            y_test_prob = search.predict_proba(X_df_test)[:,1]\n",
    "            y_test_prob = aggregate_scores(np.array(groups_test), y_test_prob, operation = np.mean)\n",
    "            y_test_pred = (y_test_prob > 0.5)*1\n",
    "\n",
    "            y_test_true = aggregate_scores(np.array(groups_test), labels_df_test[TARGET_LABEL].to_numpy(), \n",
    "                                           operation = 'most_common')       \n",
    "\n",
    "            accuracy[b] = accuracy_score(y_test_true, y_test_pred)\n",
    "            f1_macro[b] = f1_score(y_test_true, y_test_pred, average='macro', zero_division=0)\n",
    "            bal_acc[b] = balanced_accuracy_score(y_test_true, y_test_pred)\n",
    "            auroc[b] = roc_auc_score(y_test_true, y_test_prob)\n",
    "            precision, recall, thresholds = precision_recall_curve(y_test_true, y_test_prob)\n",
    "            aupr[b] = auc(recall, precision)\n",
    "            \n",
    "            fpr, fnr, _ = det_curve(y_test_true, y_test_prob)\n",
    "            tpr = 1 - fnr\n",
    "            fpr_at_k = {}\n",
    "            ks = [0.75, 0.80, 0.90, 0.95, 0.99]\n",
    "            for k in ks:\n",
    "                ind = np.argmin(np.abs(tpr - k))\n",
    "                fpr_at_k['FPR at ' + str(k)] = np.round(fpr[ind], 3)\n",
    "            \n",
    "            fpr_at_75[b] = fpr_at_k['FPR at 0.75']\n",
    "            fpr_at_80[b] = fpr_at_k['FPR at 0.8']\n",
    "            fpr_at_90[b] = fpr_at_k['FPR at 0.9']\n",
    "            fpr_at_95[b] = fpr_at_k['FPR at 0.95']\n",
    "            fpr_at_99[b] = fpr_at_k['FPR at 0.99']\n",
    "            \n",
    "            brier[b] = np.mean((y_test_prob - y_test_true)**2)\n",
    "\n",
    "metric_names = ['Accuracy', 'F1_macro', 'Balanced accuracy', 'AUROC', 'AUPR', 'FPR at 75', 'FPR at 80', \n",
    "                'FPR at 90', 'FPR at 95', 'FPR at 99', 'Brier score']\n",
    "if VALIDATE:\n",
    "    print(f'\\n{K_FOLDS}-FOLD CROSS VALIDATION RESULTS: RANDOM FOREST')\n",
    "    print()\n",
    "    agg_results_df = pd.DataFrame([np.mean(results, axis=0), np.std(results, axis=0)], \n",
    "                                  columns=metric_names, index=[\"mean\", \"std\"])\n",
    "    print(agg_results_df)      \n",
    "else:\n",
    "    results_df = pd.DataFrame([np.array([accuracy.sum()/B, f1_macro.sum()/B, bal_acc.sum()/B, \n",
    "                                         auroc.sum()/B, aupr.sum()/B], fpr_at_75.sum()/B, fpr_at_80.sum()/B,\n",
    "                                         fpr_at_90.sum()/B, fpr_at_95.sum()/B, fpr_at_99.sum()/B, \n",
    "                                         brier.sum()/B)], \n",
    "                              columns=metric_names, index=['test set'])\n",
    "    print(str(np.round(accuracy.mean(), 3)) + '+/-' + str(np.round(accuracy.std(), 3)) + '; ' + \n",
    "          str(np.round(f1_macro.mean(), 3)) + '+/-' + str(np.round(f1_macro.std(), 3)) + '; ' +\n",
    "          str(np.round(bal_acc.mean(), 3)) + '+/-' + str(np.round(bal_acc.std(), 3)) + '; ' + \n",
    "          str(np.round(auroc.mean(), 3)) + '+/-' + str(np.round(auroc.std(), 3)) + '; ' +\n",
    "          str(np.round(aupr.mean(), 3)) + '+/-' + str(np.round(aupr.std(), 3)) + '; ' + \n",
    "          str(np.round(fpr_at_75.mean(), 3)) + '+/-' + str(np.round(fpr_at_75.std(), 3)) + '; ' + \n",
    "          str(np.round(fpr_at_80.mean(), 3)) + '+/-' + str(np.round(fpr_at_80.std(), 3)) + '; ' +\n",
    "          str(np.round(fpr_at_90.mean(), 3)) + '+/-' + str(np.round(fpr_at_90.std(), 3)) + '; ' +\n",
    "          str(np.round(fpr_at_95.mean(), 3)) + '+/-' + str(np.round(fpr_at_95.std(), 3)) + '; ' +\n",
    "          str(np.round(fpr_at_99.mean(), 3)) + '+/-' + str(np.round(fpr_at_99.std(), 3)) + '; ' +\n",
    "          str(np.round(brier.mean(), 3)) + '+/-' + str(np.round(brier.std(), 3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
